{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (12009, 21)\n",
      "y_train shape: (12009,)\n",
      "X_test shape: (3003, 21)\n",
      "y_test shape: (3003,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed data from the CSV file\n",
    "data = pd.read_csv(r\"D:\\major project\\undersampled_processed_data.csv\")\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop([\"is_fraud\"], axis=1)\n",
    "y = data[\"is_fraud\"]\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Save the resulting datasets to CSV files\n",
    "X_train.to_csv('D:\\major project\\X_train.csv', index=False)\n",
    "y_train.to_csv('D:\\major project\\y_train.csv', index=False)\n",
    "X_test.to_csv('D:\\major project\\X_test.csv', index=False)\n",
    "y_test.to_csv('D:\\major project\\y_test.csv', index=False)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1440   67]\n",
      " [  54 1442]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1507\n",
      "           1       0.96      0.96      0.96      1496\n",
      "\n",
      "    accuracy                           0.96      3003\n",
      "   macro avg       0.96      0.96      0.96      3003\n",
      "weighted avg       0.96      0.96      0.96      3003\n",
      "\n",
      "\n",
      "Accuracy Score:\n",
      "0.9597069597069597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Load the training and testing datasets\n",
    "# X_train = pd.read_csv('link')\n",
    "# y_train = pd.read_csv('link')\n",
    "# X_test = pd.read_csv('link')\n",
    "# y_test = pd.read_csv('link')\n",
    "\n",
    "# Perform one-hot encoding on categorical columns\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "\n",
    "# Align the columns of X_test to match X_train (in case some categories are missing in the test set)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nAccuracy Score:\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to D:\\major project\\models\\dtc_model.pkl\n",
      "Evaluation results saved to D:\\major project\\results\\dtc_results.txt\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Save the model\n",
    "model_path = r'D:\\major project\\models\\dtc_model.pkl'\n",
    "joblib.dump(dtc, model_path)\n",
    "\n",
    "# Save the evaluation results\n",
    "evaluation_path = r'D:\\major project\\results\\dtc_results.txt'\n",
    "with open(evaluation_path, 'w') as f:\n",
    "    f.write(\"Accuracy Score:\")\n",
    "    f.write(str(accuracy) + \"\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(conf_matrix) + \"\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(class_report + \"\\n\")\n",
    "    \n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"Evaluation results saved to {evaluation_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1459   48]\n",
      " [  62 1434]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      1507\n",
      "           1       0.97      0.96      0.96      1496\n",
      "\n",
      "    accuracy                           0.96      3003\n",
      "   macro avg       0.96      0.96      0.96      3003\n",
      "weighted avg       0.96      0.96      0.96      3003\n",
      "\n",
      "\n",
      "Accuracy Score:\n",
      "0.9633699633699634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_test,y_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nAccuracy Score:\")\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to D:\\major project\\models\\rfc_model.pkl\n",
      "Evaluation results saved to D:\\major project\\results\\rfc_results.txt\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Save the model\n",
    "\n",
    "model_path = r'D:\\major project\\models\\rfc_model.pkl'\n",
    "joblib.dump(rfc, model_path)\n",
    "\n",
    "# Save the evaluation results\n",
    "evaluation_path = r'D:\\major project\\results\\rfc_results.txt'\n",
    "with open(evaluation_path, 'w') as f:\n",
    "    f.write(\"Accuracy Score: \")\n",
    "    f.write(str(accuracy) + \"\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(conf_matix) + \"\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(class_report + \"\\n\")\n",
    "    \n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"Evaluation results saved to {evaluation_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1420   87]\n",
      " [  76 1420]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95      1507\n",
      "           1       0.94      0.95      0.95      1496\n",
      "\n",
      "    accuracy                           0.95      3003\n",
      "   macro avg       0.95      0.95      0.95      3003\n",
      "weighted avg       0.95      0.95      0.95      3003\n",
      "\n",
      "\n",
      "Accuracy Score:\n",
      "0.9457209457209457\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Gradient Boosting Classifier\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nAccuracy Score:\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to D:\\major project\\models\\gbc_model.pkl\n",
      "Evaluation results saved to D:\\major project\\results\\gbc_results.txt\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Save the model\n",
    "model_path = r'D:\\major project\\models\\gbc_model.pkl'\n",
    "joblib.dump(gbc, model_path)\n",
    "\n",
    "# Save the evaluation results\n",
    "evaluation_path = r'D:\\major project\\results\\gbc_results.txt'\n",
    "with open(evaluation_path, 'w') as f:\n",
    "    f.write(\"Accuracy Score: \")\n",
    "    f.write(str(accuracy) + \"\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(conf_matix) + \"\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(class_report + \"\\n\")\n",
    "    \n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"Evaluation results saved to {evaluation_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file with the highest accuracy is D:\\major project\\results\\rfc_results.txt with an accuracy of 0.9633699633699634\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def model_selection():\n",
    "    file_paths = [\n",
    "        r'D:\\major project\\results\\dtc_results.txt',\n",
    "        r'D:\\major project\\results\\rfc_results.txt',\n",
    "        r'D:\\major project\\results\\gbc_results.txt'\n",
    "    ]\n",
    "\n",
    "    max_accuracy = -1.0  # Initialize with a very low value\n",
    "    best_file = None\n",
    "    best_model = None\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_contents = file.read()\n",
    "\n",
    "        # Find the accuracy in the file\n",
    "        accuracy = None\n",
    "        for line in file_contents.split('\\n'):\n",
    "            if line.startswith(\"Accuracy Score:\"):\n",
    "                accuracy = float(line.split(\":\")[1].strip())\n",
    "\n",
    "        # Update if a higher accuracy is found\n",
    "        if accuracy is not None and accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            best_file = file_path\n",
    "\n",
    "    print(f\"The file with the highest accuracy is {best_file} with an accuracy of {max_accuracy}\")\n",
    "\n",
    "    if best_file == r'D:\\major project\\results\\gbc_results.txt':\n",
    "        best_model = r'D:\\major project\\models\\gbc_model.pkl'\n",
    "    elif best_file == r'D:\\major project\\results\\rfc_results.txt':\n",
    "        best_model =  r'D:\\major project\\models\\rfc_model.pkl'\n",
    "    elif best_file == r'D:\\major project\\results\\dtc_results.txt':\n",
    "        best_model =  r'D:\\major project\\models\\dtc_model.pkl'\n",
    "    else:\n",
    "        raise ValueError(\"No valid model found\")\n",
    "\n",
    "    final_model = r'D:\\major project\\models\\final_model.pkl'\n",
    "    shutil.copy(best_model , final_model)\n",
    "\n",
    "model_selection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
